# End-to-end Multi-Modal Multi-Task Vehicle Control for Self-Driving Cars with Visual Perceptions
## 摘要
* 背景：之前的端到端的方法，以单幅图像或者图像序列为输入，利用CNN预测方向盘转角。虽然只预测方向盘转角的方法可以获得较好的效果，但是只有方向盘转角是不能满足车辆控制需求的。
* 方法：该论文提出一种多任务的学习框架，利用端到端的方案同时预测方向盘转角和速度控制。由于直接预测速度值非常困难，该论文首先提出使用网络，根据图像序列，预测离散的速度命令和方向盘转角。此外，改论文还提出一种多模态多任务的网络，以之前的速度反馈和视觉信息为输入。
> 注：其中多模态指的是不同类型的输入，这里是指速度反馈和视觉图像两种不同模态的输入。
* 实验：在Udacity数据集和新采集的SAIC数据集上进行了实验。

## 网络结构
### 方向盘转角模型

![](https://github.com/sgding/Paper-list/blob/master/planning/end-to-end/pictures/steering-image.png) 

* 该模型共分为9层，包括5个卷积层和4个全连接层
* 改进点：（1）使用较大的卷积核；（2）改变图片的宽高比例为1：1，之前的方法为2.5：1。根据人的直觉，沿着y轴方向上的视觉信息分布对方向角预测更有用，因此CNN的卷积核的宽度应该大于高度。因此，为了简化，改变图片的长宽比例，且仍采用宽高一样的卷积核。

### 速度命令网络

* 以图像序列为输入，同时预测速度命令和方向盘转角
* 卷积层结构同上，编码的特征喂给LSTM层，最后输出的特征用来做方向角回归和速度控制分类

### 多模态多任务网络

* 包含一个视觉编码器和一个速度编码器，其中视觉编码器只使用一帧作为输入，可以提供FPS；而速度编码器使用之前反馈的速度序列。
